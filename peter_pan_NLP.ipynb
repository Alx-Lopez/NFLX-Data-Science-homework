{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "peter pan NLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOfYHGawWJtVjyutq+ODDXU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alx-Lopez/NFLX-Data-Science-homework/blob/main/peter_pan_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "The book cover of Peter and Wendy\n",
        "\n",
        "Fly away with Peter Pan!\n",
        "Peter Pan has been the companion of many children, and went a long way, starting as a Christmas play and ending up as a Disney classic. Did you know that although the play was titled \"Peter Pan, Or The Boy Who Wouldn't Grow Up\", J. M. Barrie's novel was actually titled \"Peter and Wendy\"?\n",
        "\n",
        "You're going to explore and analyze Peter Pan's text to answer the question in the instruction pane below. You are working with the text version available here at Project Gutenberg. Feel free to add as many cells as necessary. Finally, remember that you are only tested on your answer, not on the methods you use to arrive at the answer!\n",
        "\n",
        "Note: If you haven't completed a DataCamp project before you should check out the Intro to Projects first to learn about the interface. Intermediate Importing Data in Python and Introduction to Natural Language Processing in Python teach the skills required to complete this project. Should you decide to use them, English stopwords have been downloaded from nltk and are available for you in your environment."
      ],
      "metadata": {
        "id": "b02ZDE0-Zk9R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62CmalQLZHsk",
        "outputId": "6f353094-3219-4504-a153-9621e8227969"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Use this cell to begin your analysis, and add as many as you would like!\n",
        "from urllib.request import urlopen,Request\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://www.gutenberg.org/files/16/16-h/16-h.htm'\n",
        "request = Request(url)\n",
        "response = urlopen(request)\n",
        "html = response.read()\n",
        "response.close()"
      ],
      "metadata": {
        "id": "vvzW2Ec5ZNve"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using BeautifulSoup to organize clean and organize data\n",
        "soup = BeautifulSoup(html)\n",
        "p_tags = soup.find_all(\"p\")\n",
        "text_list = \"\"\n",
        "for para in p_tags:\n",
        "    text_list += para.getText()"
      ],
      "metadata": {
        "id": "5WUwbwiJZSvc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using the nltk to count the words\n",
        "tokens = word_tokenize(text_list)\n",
        "lower_tokens = [t.lower() for t in tokens]"
      ],
      "metadata": {
        "id": "A-nXGTyyZThO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nltk.download('stopwords')\n",
        "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
        "no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
        "\n",
        "count = Counter(no_stops).most_common(10)"
      ],
      "metadata": {
        "id": "82tNjot5ZV02"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6SyRsdbaua5",
        "outputId": "b273c9f6-0cd3-4743-ea25-e57db63bbbb5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('peter', 392), ('said', 358), ('wendy', 350), ('would', 216), ('one', 209), ('hook', 169), ('could', 141), ('cried', 136), ('john', 133), ('darling', 117)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "protagonists = ['peter','wendy','hook','john']"
      ],
      "metadata": {
        "id": "7uKCUh1HaxEu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}